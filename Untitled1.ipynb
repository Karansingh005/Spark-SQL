{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bc37a15-afc7-4f10-b038-76b0707881a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/16 01:26:34 WARN Utils: Your hostname, karan resolves to a loopback address: 127.0.1.1; using 192.168.244.130 instead (on interface ens33)\n",
      "23/11/16 01:26:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/karan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/karan/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0b74e354-ea8d-4570-abac-eb0cd68c9264;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 308ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0b74e354-ea8d-4570-abac-eb0cd68c9264\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/6ms)\n",
      "23/11/16 01:26:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#!pip install findspark\n",
    "#!pip install delta-spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf88fd8-4f20-4c95-bb0e-c903313cccca",
   "metadata": {},
   "source": [
    "## Managed Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d60b5cb5-bddf-41e0-a193-e567f8c02d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/home/karan/Downloads/output_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628b80c2-d45d-4b60-809c-0e3240854611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/16 01:26:54 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"reddit1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b1b657-c1fe-4fbb-b239-ab9f591663e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='reddit1', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb13f0c8-ba5c-4d1a-9c0a-83c914d3270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------+----+--------------------+----------------+----------+\n",
      "|               title|upvotes|comments|time|              author|       subreddit|    thread|\n",
      "+--------------------+-------+--------+----+--------------------+----------------+----------+\n",
      "|Weekly Entering &...|      5|      91| 120|       AutoModerator|self.datascience|      Meta|\n",
      "|    self.datascience|     43|      30|   7|    Substantial_Page|self.datascience|Discussion|\n",
      "|Ex manager sort o...|     42|      39|   9|       ssiddharth408|self.datascience|Discussion|\n",
      "|    self.datascience|     37|      46|   8|            saabiiii|self.datascience|Networking|\n",
      "|Is pytorch not go...|      0|       4|  24|     _CynicalCyanide|self.datascience|Discussion|\n",
      "|    self.datascience|     11|       9|   1|    Ok-Cucumber-3932|self.datascience|Discussion|\n",
      "|Feeling lost and ...|     77|      71|   5|Accomplished_Ad_5697|self.datascience|Discussion|\n",
      "|    self.datascience|      7|       2|  18|  Odd_Discipline9354|self.datascience|Discussion|\n",
      "|Where do the data...|     11|      12|   6|             X_Drake|self.datascience|Discussion|\n",
      "|    self.datascience|      0|       1|  24|Massive_Shoulder_386|self.datascience|Discussion|\n",
      "|Math and comp sci...|    237|     115|  12|InevitableTraining69|self.datascience|Discussion|\n",
      "|    self.datascience|     25|       9|   1|        abelEngineer|self.datascience|Discussion|\n",
      "|Why should I lear...|      2|      15|  24|            ALESS885|self.datascience|Discussion|\n",
      "|    self.datascience|      0|       0|  19|              asxvi_|self.datascience|   Tooling|\n",
      "|Is handling error...|      3|       1|   8|             paddy_m|self.datascience|  Projects|\n",
      "|    self.datascience|      1|       0|   3|         bennymac111|self.datascience|Discussion|\n",
      "|How often do comp...|      4|       1|   6| Total-Opposite-8396|self.datascience|Discussion|\n",
      "|    self.datascience|      0|       0|   3|FedericoCalo07051998|self.datascience|Discussion|\n",
      "|Suggest me a Spat...|      0|       2|  14|       LifeisWeird11|self.datascience|Discussion|\n",
      "|    self.datascience|     15|      20|   7|          sshaginyan|self.datascience|   Tooling|\n",
      "+--------------------+-------+--------+----+--------------------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM reddit1;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72815580-7369-4f08-a305-602db059e91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|               title|              string|   NULL|\n",
      "|             upvotes|              string|   NULL|\n",
      "|            comments|              string|   NULL|\n",
      "|                time|              string|   NULL|\n",
      "|              author|              string|   NULL|\n",
      "|           subreddit|              string|   NULL|\n",
      "|              thread|              string|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|                Name|spark_catalog.def...|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Location|file:/home/karan/...|       |\n",
      "|            Provider|               delta|       |\n",
      "|    Table Properties|[delta.minReaderV...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED reddit1;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0110ff3-b04e-47ef-a9fa-3eecb300b1bb",
   "metadata": {},
   "source": [
    "## Unmanaged Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef11cffe-5d52-41f6-9d33-299ff9e30e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.repartition(2).write.format(\"delta\").mode(\"overwrite\").\\\n",
    "option(\"path\", \"Files/reddit2\")\\\n",
    ".saveAsTable(\"reddit2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7661af7-8800-4b19-adfd-74a59897c8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='reddit1', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='reddit2', catalog='spark_catalog', namespace=['default'], description=None, tableType='EXTERNAL', isTemporary=False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95d014e5-6693-4934-86db-670609f9fd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------+----+--------------------+----------------+----------+\n",
      "|               title|upvotes|comments|time|              author|       subreddit|    thread|\n",
      "+--------------------+-------+--------+----+--------------------+----------------+----------+\n",
      "|    self.datascience|      8|      10|   1|              ml_dnn|self.datascience|Discussion|\n",
      "|    self.datascience|      1|       1|   1|               Tpy26|self.datascience|Discussion|\n",
      "|    self.datascience|     13|      21| 240|        DapperAd8264|self.datascience|Discussion|\n",
      "|Data science/anal...|      9|      32| 144|     Much-Focus-1408|self.datascience|Discussion|\n",
      "|Data Science for ...|      4|       1|   1|Inevitable-Quality15|self.datascience|    Career|\n",
      "|    self.datascience|      1|       1|   1|Commercial_Sympathy3|self.datascience| Education|\n",
      "|    self.datascience|      0|       6| 624|       BlackLands123|self.datascience|   Tooling|\n",
      "|    self.datascience|      3|       2|   1|       Free-Task8814|self.datascience|    Career|\n",
      "|    self.datascience|      1|       3|   1|            m_eowski|self.datascience|  Projects|\n",
      "|    Markdown To HTML|     70|      63| 240|           databro92|self.datascience|Discussion|\n",
      "|Unpopular Opinion...|     24|      87| 480|   Excellent_Cost170|self.datascience|Discussion|\n",
      "|    self.datascience|     81|     107|   1|         perguntando|self.datascience|Discussion|\n",
      "|    self.datascience|      2|       3| 624|             Senande|self.datascience| Education|\n",
      "|    self.datascience|      2|       1| 648|            Bhagafat|self.datascience|    Career|\n",
      "|    self.datascience|     79|      45| 384|       nondualist369|       i.redd.it|  Projects|\n",
      "|    self.datascience|     17|      14| 360|       RightProfile0|self.datascience|  Projects|\n",
      "|What would you ge...|      0|      22| 576|        Old-Food2140|self.datascience| Education|\n",
      "|Is there no hope ...|      3|       6|   1|   Good-Concern-9732|self.datascience|   Tooling|\n",
      "|    self.datascience|      0|       3|   1|       oniongarlic88|self.datascience|Discussion|\n",
      "|    self.datascience|      0|       0|  24|           Nah_Maaan|self.datascience| Education|\n",
      "+--------------------+-------+--------+----+--------------------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM reddit2;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ac8c0d9-4f75-43e6-a73b-3e2430fbbd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|               title|              string|   NULL|\n",
      "|             upvotes|              string|   NULL|\n",
      "|            comments|              string|   NULL|\n",
      "|                time|              string|   NULL|\n",
      "|              author|              string|   NULL|\n",
      "|           subreddit|              string|   NULL|\n",
      "|              thread|              string|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|                Name|spark_catalog.def...|       |\n",
      "|                Type|            EXTERNAL|       |\n",
      "|            Location|file:/home/karan/...|       |\n",
      "|            Provider|               delta|       |\n",
      "|    Table Properties|[delta.minReaderV...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED reddit2;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d3eed9-ebbf-4616-a56f-7485f343e46a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
